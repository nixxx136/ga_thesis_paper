\chapter{Background}
\label{chap:background}

\section{Background}
Based on the previous work of people using NSGA-II on some multi-objective optimization problems\cite{Magnier_2010_Multiobjective}, we believe we can also applied NSGA-II on our Urban Design Model to find the optimal solution. 

\section{Previous Work}

\subsection{Genetic Algorithm(GA)}
After the Genetic Algorithm was introduced in 1975 by John Henry Holland\cite{Holland_1975_Book}, it has been applied in many areas. With mimicking the process of natural selection, GA uses selection, crossover, and mutation to generate solutions and tries to find the optimal solution for our problems. More specifically, we have an initial population size, then based on our criterion or objective, we give each individual a fitness value. After we select a certain number of individuals from our population based on their fitness, then we apply our crossover function with a crossover probability to have some new offsprings. Next, we apply the mutation function with a mutation probability on the new offsprings. After all those steps, hopefully we can  have a new population which have some better fitness values. Last, we repeat our steps until we achieve our optimal solution.

\subsection{Multiobjective Genetic Algorithm(MOGA)}
When we apply GA to find our optimal solutions, we can divide those problems into two categories based on the number of objectives: Single Objective problems and Multi-Objective problems. Let's take a simple example for single objectives first. For example we want to find the minimum value of \(y=x^2\) with \(x\in [-3,3]\), it is easy to see that we could achieve the minimun value at \(x=0\). When we have two objectives, the problem becomes more complicated. For example we want to minimize both \(y_{1}=x^2\) and \(y_{2}=(x-2)^2\). Then, we are not able to find a specific \(x\) value for this problem. From \autoref{fig:two_functions}, we could see that \(x\) at 0 could give us minimum value of \(y_{1}=x^2\) and \(x\) at 2 could give us minimum value of \(y_{2}=(x-2)^2\). However we are not able to find a \(x\) value which could guarantee us both \(y_{1}\) and \(y_{2}\) at their minimun value. We call \(x=\) \(0\) or \(2\) Pareto Optimal Solutions\cite{Hans_1988_Multicriteria_Pareto_Optimal}\cite{Vira_1983_Multiobjective_Pareto_Optimal}. You could find more information about Pareto Optimal Solutions at our latter sections. Other than \(x=\) \(0\) or \(2\), any points between \(0\) and \(2\) could be a valid compromise solution. 

\begin{figure}[htp] 
\centering
\includegraphics[scale=.2]{images/Figure_1.png}
\caption{Functions \(y_{1}=x^2\) and \(y_{2}=(x-2)^2\)}
\label{fig:two_functions}
\end{figure}

In the real word, there are many kinds of simultaneous optimization of multiple objectives problems. People have tried many methods to find Pareto Optimal Solution. The classical method is to convert multiobjective problem to a single objective problem. In paper \cite{NSGA_1994}, three classical methods are introduced: 1. Objective Weighting, 2. Distance Functions, and 3. Min-Max Formulation. All those methods have one common drawback: it can only give us a single-point solution. However, when decision makers make decision, they often need different alternatives. Another significant drawback of those method is they heavily depend on what weight vector or demand level we choose, which demands that the user have knowledge about the underlying problem. However, in most cases, users do not have much knowledge about the problem and that is one of the reasons they use genetic algorithm. So, those methods are not efficiency and we probably need to try many times to get a acceptable solution.

Since classical methods to solve multiobjective optimization problems are inadequate and inconvenient, people start to find other ways to implement GA. Next, we will briefly introduce some of those implementations.

\subsection{Vector Evaluated Genetic Algorithm(VEGA)}
VEGA is the first practical algorithm and was developed by Schaffer in 1984\cite{Schaffer_1984_Some}. Instead of changing multiple objectives problem to a single objective problem, Schaffer modified the simple tripartite genetic algorithm by performing independent selection cycles according to each objective\cite{Schaffer_1984_Some}. Bascially, to fill up a portion of the mating pool he created a loop around the traditional selection procedure to repeated selection method for each individual objective. After this selection step, we could have a thoroughly shuffled population to apply crossover and mutation steps. This could make the mating of individuals from different subpopulation groups possible.

With the independent selection of specialists, we could make the population in speciation. After a large number of generations, we could see the convergence of the entire population toward the individual optimum regions. To make a decision, we may not want to have any bias on such middling individuals, rather, we desire more nondominated points.Schaffer developed two heuristics to minimize this speciation: the nondominated selection heuristic and the mate selection heuristic. For the nondominated selection heuristic, he penalized the dominated individuals by substracting a small fixed penalty from their expected number of copies druing selection. Then the total penalty for dominated individuals was divided among the nondominated individuals and was added to their expected number of copies during selection. For the mate selection heuristic, he wanted to promoted the cross-breeding of specialists from different subgroups by selecting an individual, as a mate to a randomly selected individual, which has the maximum Euclidean distance in the performance space from its mate. However, both of those two heuristics are failed due to some certain problems.

\subsection{Nondominated Sorting Genetic Algorithm(NSGA)}
NSGA was first introduced by N. Srinivas and Kalyanmoy Deb in 1994\cite{NSGA_1994}. They investigated Goldberg's notion of nondominated sorting in GAs along with a niche and speciation method to find multiple Pareto Optimal Solutions simultanesously. The reason they call this algorithm the Nondominated Sorting Genetic Algorithm is that it build on a nondominated sorting procedure. Two ideas behind the nondominated sorting procedure are ranking selection which is used to emphasize good points method and niche method which is used to maintain stable subpopulations of good points.

The only difference between simple genetic algorithm and NSGA is the way selection operator works. First, we rank the population based on the individuals' nondomination and identify those nondominated individuals from the current population. Second, let all those individuals constitute the first nondominated front and assign a large dummy fitness value to them. All those nondominated individuals which have an equal reproductive potential should have same fitness value. Third, to maintain diversity in the population, we apply sharing methods\cite{Deb_1989_Investigation}\cite{Deb_1989_Genetic} on these classified individuals. Sharing is achieved by performing selection operation using degraded fitness values that are obtained by dividing the original fitness value of an individual by a quantity proportional to the number of individuals around it. Last, ignore those nondominated multiple optimal points and using the same method to process the rest of population to get the second nondominated front. This time, we assign a new dummy fitness value to the second nondominated front which is smaller than the first nondominated front. By this process, we could classified the entire population into several fronts. After the classification process, we reproduct the population by each individual's dummy fitness value and apply a stochastic remainder proportionate seletion on them.

After this selection process, we move to crossover operation. Since we want to find nondominated regions or Pareto Optimal fronts and first front have the maximum fitness value, we give them more copies. This could give us a fast convergence of the population toward nondominated regions. Furthermore, sharing method could help us to distribute the population over this region. By nondominated sorting procedure, NSGA becomes more efficient. And NSGA can solve both minimization and maximization problems. But it is still not very handy.

\subsection{Nondominated Sorting Genetic Algorithm II(NSGA-II)}
NSGA-II\cite{NSGA-II} is a very famous multi-objective optimization algorithm which is wide used  to find the optimal solution nowadays. Compare with the previous NSGA, NSGA-II has improved the computational efficiency, elitism, and sharing parameter. For computational complexity of nondominated sorting, it improve from \(O(MN^{3})\) to \(O(MN^{2})\). With the elitism, NSGA-II could not only speed up the GA performance, but also prevent loss of good solutions. Moreover, NSGA II does not need to specify a sharing parameter \(\sigma_{share}\), which is a requirement for pervious multi-objective evolutionary algorithms.

\subsection{Pareto Optimal Solution}

